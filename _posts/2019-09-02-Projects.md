---
layout: post
title: Thesis projects
date:   2019-09-03
author: Katia
categories: Info
mathjax: true
---

Hello everyone,

NLP researchers at UvA have proposed some exciting thesis projects for you! You can find them below. If you are interested in any of the projects, please contact the prospective supervisor directly.

###Probing for typological properties in multilingual word and sentence representations 

Supervisor: Ekaterina Shutova 

Collaborators: Douwe Kiela (Facebook AI Research)

####Description

Languages may share universal features at a deep, abstract level, but the structures found in real-world, surface-level natural language vary significantly. This variation makes it challenging to transfer NLP models across languages or to develop systems that apply to a wide range of languages. As a consequence, the availability of NLP technology is limited to a handful of resource-rich languages, leaving many other languages behind. Understanding linguistic variation in a systematic way is crucial for the development of effective multilingual NLP applications, thus making NLP technology more accessible globally. 

In recent years, much NLP research has focused on the development of multilingual models, typically based on multilingual joint learning or zero-shot transfer learning across languages. Much of this research has utilized multilingual word or sentence representations, that project words or sentences in multiple languages into a shared multilingual semantic space. Such models abstract over language-specific features and represent words and sentences from multiple languages in a language-agnostic manner, such that words or sentences with similar meanings (regardless of an actual language) obtain similar representations. While this work has met with success, enabling effective model transfer across languages, little is known about the linguistic properties of individual languages that such representations encode. The goal of this project is to investigate to what extent multilingual word and sentence representations capture the patterns of cross-lingual similarity and variation. We will use techniques from the rapidly growing field of interpretation of neural networks (e.g. Tenney et al. 2019) to design methods for probing state-of-the-art multilingual contextualised word representations and sentence encoders, such as LASER (Artexte and Schwenk, 2018), multilingual BERT (Devlin et al., 2019) or XLM (Lample and Conneau, 2019), for a range of language characteristics. We will thus investigate to what extent the above models encode typological features pertaining to syntactic and semantic structure, how these are represented within the network and generalised across language families. We will also look at whether these models can be used to automatically induce the patterns of linguistic variation across language families.

This is an ambitious project, suitable for students with a background and interest in machine learning and keen to conduct novel research. We hope that it would lead to a publication.

####What are our expectations of the student?

Independent and proactive attitude and an interest in artificial intelligence
Solid maths background: calculus, linear algebra, probability and statistics
Advanced programming skills (algorithms and data structures; ideally experience with Pytorch or other deep learning libraries)
Knowledge and skill in developing and applying machine learning algorithms (particularly, interest and experience in deep learning) 
Good familiarity with and experience in NLP; but donâ€™t worry we will help you to fill in the gaps.

####Further reading:

Edoardo Ponti, Helen O'Horan, Yevgeni Berzak, Ivan Vulic, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, Anna Korhonen. 2018. Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing.  In arXiv e-prints, abs/1807.00914. 

Mikel Artetxe and Holger Schwenk. 2018. Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond. ArXiv.

Tal Schuster, Ori Ram, Regina Barzilay, Amir Globerson. Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing. In Proceedings of NAACL 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of NAACL 2019.

Guillaume Lample and Alexis Conneau. 2019. Cross-lingual Language Model Pretraining. ArXiv.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for sentence structure in contextualized word representations. Proceedings of ICLR 2019.

