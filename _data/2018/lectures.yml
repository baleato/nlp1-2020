-
  layout: lecture
  selected: y
  date: 2020-10-28
  img: Live-1
  uid: intro
  title: "Live lecture: Introduction"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "Introduction to the course"
  background:
  discussion:
  slides: 
  video: 
  further: 
    - "Chapter 4: [Naive Bayes classification and sentiment](https://web.stanford.edu/~jurafsky/slp3/4.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 1 (released on 2020-10-28)
  img: PoS
  uid: lec2
  title: "Language models and part-of-speech tagging"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss language models, i.e. modelling word sequences, and part-of-speech tagging"
  background:
  discussion:
  slides: 
  video: 
  further: 
    - "Chapter 3: [Language modelling with n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 8: [Part-of-speech tagging](https://web.stanford.edu/~jurafsky/slp3/8.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:
-  
  layout: lecture
  selected: y
  date: 2020-11-04
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Language models and part-of-speech tagging"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss language models and part-of-speech tagging."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:     
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Morphology
  uid: lec3
  title: "Modelling structure: morphology and syntax"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss morphological processing and syntactic parsing"
  background:
  discussion:
  slides: 
  video: 
  further: 
    - "Lecture notes on moprphology are available [here](https://cl-illc.github.io/nlp1/resources/slides/Morphology-notes.pdf)"
    - "Chapter 12: [Constituency grammars](https://web.stanford.edu/~jurafsky/slp3/12.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: Week 2 (released on 2020-11-04)
  img: Parsing
  uid: lec4
  title: "Syntactic parsing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss syntactic parsing."
  background:
  discussion:
  slides: 
  video: 
  further: 
     - "Chapter 13: [Constituency parsing](https://web.stanford.edu/~jurafsky/slp3/13.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: 2020-11-11
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Morphology and syntax"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss morphological processing and syntactic parsing."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: WordNet
  uid: lec4
  title: "Lexical semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss lexical semantics, i.e. modelling the meaning of words."
  background:
  discussion:
  slides: 
  video: 
  further: 
     - "Chapter 19: [Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/19.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: vectors
  uid: lec5
  title: "Distributional semantics"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will introduce statistical models of word meaning"
  background:
  discussion:
  slides: 
  video: 
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 3 (released on 2020-11-11)
  img: skip-gram
  uid: lec6
  title: "Generalisation and word embeddings"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss generalisation from words to semantic classes and learning dense vector representations - word embeddings."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 6: [Vector semantics and embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in Jurafsky and Martin (3rd edition)."
    - "A gentle introduction to neural networks can be found [here](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
    - "The following paper provides a nice explanation of skip-gram with negative sampling: Yoav Goldberg and Omer Levy. [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)"
  video: 
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2020-11-18
  img: Live-1
  uid: lec7
  title: "Live lecture: Compositional semantics and sentence representations"
  instructor: "Mario Giulianelli"
  note: 
  abstract: >
    "In this lecture, we will discuss compositional semantics, i.e. modelling the meaning of phrases and sentences, and learning neural representations of sentences."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 7: [Neural networks and neural language models](https://web.stanford.edu/~jurafsky/slp3/7.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 9: [Sequence processing with recurrent neural networks](https://web.stanford.edu/~jurafsky/slp3/9.pdf) in Jurafsky and Martin (3rd edition)."
    - "A good and general reference for Neural Networks in NLP: Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](https://arxiv.org/abs/1510.00726)"
    - "A gentle introduction to LSTMs is available [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
    - "This is one of the papers that have introduced tree LSTM models: Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf)" 
  video: 
  code: 
  data: 
-  
  layout: lecture
  selected: y
  date: 2020-11-25
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Semantics"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss lexical and distributional semantics."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: Discourse
  uid: lec8
  title: "Discourse processing"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will discuss discourse processing, i.e. modelling larger text fragments."
  background:
  discussion:
  slides: 
  further: 
    - "Chapter 22: [Coreference resolution](https://web.stanford.edu/~jurafsky/slp3/22.pdf) in Jurafsky and Martin (3rd edition)."
    - "Chapter 23: [Discourse coherence](https://web.stanford.edu/~jurafsky/slp3/23.pdf) in Jurafsky and Martin (3rd edition)." 
  video: 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 5 (released on 2020-11-25)
  img: dialogue
  uid: lec9
  title: "Dialogue modelling"
  instructor: "Recorded lecture by Raquel Fernandez"
  note: 
  abstract: >
    "In this guest lecture, we will introduce an important NLP application -- dialogue modelling."
  background:
  discussion:
  slides: 
  further:
    - "Chapter 26: [Dialogue systems and chatbots](https://web.stanford.edu/~jurafsky/slp3/26.pdf) in Jurafsky and Martin (3rd edition)."
  video:
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: 2020-12-02
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Discourse and dialogue"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss modelling discourse and dialogue."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: Summarization 
  uid: lec10
  title: "Language generation and summarisation"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "In this lecture, we will talk about language generation and cover a particular language generation task, text summarisation, in more detail."
  background:
  discussion:
  slides: 
  further: 
    - "A survey of recent summarisation techniques is available [here](https://arxiv.org/pdf/1804.04589.pdf)"
    - "An introduction to sequence-to-sequence models: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
  video: 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: Week 6 (released on 2020-12-02)
  img: MT
  uid: lec11
  title: "Machine translation"
  instructor: "Recorded lecture"
  note: 
  abstract: >
    "We will discuss the fundamentals of machine translation, including word-based / alignment models and phrase-based SMT"
  background:
  discussion:
  slides: 
  further: 
    - "[Explanation of IBM models 1 and 2](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf)"
    - "[Word based IBM models](http://www.aclweb.org/anthology/J93-2003)"
    - "[Phrase-based SMT](http://www.aclweb.org/anthology/N03-1017) and this [book](http://www.statmt.org/book/)"
    - "[Neural Encoder-Decoder](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"
    - "[The Annotated Encoder-Decoder blog post](https://bastings.github.io/annotated_encoder_decoder/)"
  video: 
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-12-09
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Summarisation and machine translation"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this Q & A session, we will discuss summarization and machine translation."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: Week 7 (released on 2020-12-09)
  img: bayes-nlp
  uid: lec13
  title: "Foundations of Bayesian NLP"
  instructor: "Recorded lecture by Wilker Aziz"
  note: 
  abstract: |
      In this lecture, we will discuss the differences between frequentism and Bayesian modelling. We will discuss the concept of a prior and Bayesian inference. The model we will use to illustrate concepts is the Dirichlet-Multinomial model, the base for models such as Bayesian mixture models, HMM, and LDA. For approximate inference, we will discuss MCMC and in particular Gibbs sampling.
  background:
  discussion:
  slides: resources/slides/NLP1-lecture13.pdf
  further: |
      * For a POS tagging model: [A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging](http://aclweb.org/anthology/P07-1094) 
      * For a PCFG model: [Bayesian Inference for PCFGs via Markov chain Monte Carlo](http://aclweb.org/anthology/N07-1018)
      * A very special type of mixture model: [Bayesian Word Alignment for Statistical Machine Translation](http://aclweb.org/anthology/P11-2032)
      * The classict of all times: [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
        * This paper focusses on a different type of approximate inference technique (not MCMC, but rather variational inference), in ML4NLP we cover it in great detail (in particular, this is the class of algorithms we use to do probabilistic modelling with neural networks)
      * If you are interested in Bayesian non-parametric methods for NLP, check Sharon Goldwater's [thesis](https://homepages.inf.ed.ac.uk/sgwater/papers/thesis_1spc.pdf), it's remarkably well written and clear!
  video: https://webcolleges.uva.nl/Mediasite/Play/e0d365e80e8b4756b66f68a74d3b77771d
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2020-12-11
  img: LiveQA
  uid: qa2
  title: "Live Q & A: Bayesian NLP"
  instructor: "Wilker Aziz"
  note: 
  abstract: >
    "In this Q & A session, we will discuss Bayesian NLP."
  background:
  discussion:
  slides: 
  video: 
  further:   
  code: 
  data:
